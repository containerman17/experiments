# EVM Sink - Technical Notes for AI Context

## What This Is

A Go service that ingests EVM blockchain data (blocks, receipts, traces) from RPC nodes, stores in PebbleDB, 
compacts to S3, and serves to consumers via zstd-compressed TCP streaming. Designed for multi-chain support.

## Architecture Decisions

### Storage Strategy: PebbleDB → S3

1. **PebbleDB (hot)**: All blocks land here first
   - Key format: `block:{chainID}:{blockNum:020d}` (20-digit padding for lexicographic ordering)
   - Value: JSON blob of NormalizedBlock (block + receipts + traces)
   - Keeps ~1000 blocks as buffer before compaction

2. **S3 (cold)**: Compacted historical data
   - Format: `.jsonl.zstd` (newline-delimited JSON, zstd compressed)
   - 100 blocks per file
   - Path: `{prefix}/{chainID}/{startBlock:020d}-{endBlock:020d}.jsonl.zstd`

3. **Why this split?**
   - PebbleDB for fast writes and recent block access
   - S3 for cheap, durable historical storage
   - Compaction happens in background, doesn't block ingestion

### Adaptive RPC Controller

Problem: RPC nodes have varying capacities, network latencies differ, load fluctuates.

Solution: One knob - `max_parallelism`. Everything else derived:
- `min_parallelism` = max(2, maxP/10)
- `target_latency` = 1200ms (allows 200ms ping + 1s work)
- `max_latency` = 2000ms
- `max_errors_per_min` = 10

Adjustment runs every 5s based on 60s sliding window:
- If errors > threshold: halve parallelism (aggressive backoff)
- If P95 > max_latency: reduce by 2
- If P95 < target * 0.7: increase by 1 (cautious)

### WebSocket Head Tracker

The `HeadTracker` (rpc/heads.go) subscribes to `newHeads` via WebSocket for instant block notifications:
- On startup: fetches initial block via HTTP RPC
- Then subscribes to `eth_subscribe("newHeads")` via WebSocket
- Latest block is cached in atomic - `GetLatestBlock()` is instant, no RPC call
- Auto-reconnects on disconnect with 5s delay
- **Requires** `/rpc` in URL which gets converted to `/ws` for WebSocket

### Sliding Window Block Fetcher

The `Fetcher.StreamBlocks()` uses a sliding window approach:
- Keeps `windowSize` fetches in flight simultaneously
- Processes results in order as they complete
- Polls for new blocks every 100ms when at tip
- Window size configurable via `lookahead` config (default: 100)

### TCP Streaming Protocol

**Zstd-compressed** NDJSON over TCP (not plain text!).

Client sends:
- `{"type":"list_chains"}` → get available chains
- `{"chain_id":N,"from_block":M}` → start streaming

Server sends:
- `{"type":"chains","chains":[...]}` → chain list response
- `{"type":"block","chain_id":N,"block_number":M,"data":{...}}` → block data
- `{"type":"status","status":"live","head_block":N}` → caught up notification
- `{"type":"error","message":"..."}` → errors

Both sides wrap TCP connection in zstd encoder/decoder.

### Streaming Logic (Server)

1. **Try PebbleDB first**: If block exists in hot storage, send immediately
2. **Fallback to S3**: Download 100-block batch with 10-batch lookahead prefetching
3. **At tip**: Send "live" status, poll PebbleDB every 50ms for new blocks
4. **Flush strategy**: Flush after each block at tip (low latency), after each S3 batch (good compression)

### Ingestion Logic

1. Startup: Resume from PebbleDB latest → S3 latest → block 1
2. WebSocket subscription provides instant latest block (no polling needed)
3. Sliding window fetches blocks in parallel, processes in order
4. Save to PebbleDB, notify server of new blocks
5. Log progress every 5s with blocks/sec, ETA, current parallelism

### Compaction

- Runs every 10s per chain
- Only compacts if >1100 blocks in PebbleDB (keeps 1000 as buffer)
- Aligns to 100-block boundaries (1-100, 101-200, etc.)
- Uploads to S3, then deletes from PebbleDB

## File Structure

```
evm-sink/
├── cmd/
│   ├── sink/main.go          # Entry point, config, wiring, ingestion loop
│   └── example-client/main.go # Example client with reconnection and stats
├── rpc/
│   ├── types.go              # EVM types (Block, Transaction, Receipt, etc.) + Config
│   ├── controller.go         # Adaptive parallelism controller with semaphore
│   ├── fetcher.go            # Sliding window block fetcher with batch RPC
│   └── heads.go              # WebSocket head tracker (newHeads subscription)
├── storage/
│   ├── pebble.go             # PebbleDB operations
│   ├── s3.go                 # S3 upload/download with zstd, batch helpers
│   └── compactor.go          # Background compaction
├── api/
│   └── server.go             # TCP streaming server with zstd compression
├── client/
│   └── client.go             # Go client library for consumers
├── config.yaml               # Local config (gitignored)
├── config.example.yaml       # Template
└── go.mod
```

## Config

```yaml
pebble_path: ./data/pebble
listen_addr: ":9090"
lookahead: 100  # Sliding window size for block fetching

s3_bucket: bucket-name
s3_region: auto
s3_endpoint: https://xxx.r2.cloudflarestorage.com  # R2/MinIO
s3_access_key: ...
s3_secret_key: ...
s3_prefix: v1

chains:
  - chain_id: 43114
    name: C-Chain
    rpcs:
      - url: http://node:8545/rpc  # Must contain /rpc for WebSocket conversion
        max_parallelism: 128
```

## Key Types

```go
// What we store per block
type NormalizedBlock struct {
    Block      Block                 `json:"block"`
    Traces     []TraceResultOptional `json:"traces"`
    StateDiffs []StateDiffResult     `json:"stateDiffs"`  // Currently unused
    Receipts   []Receipt             `json:"receipts"`
}

// Trace result (nil Result for precompile calls)
type TraceResultOptional struct {
    TxHash string     `json:"txHash"`
    Result *CallTrace `json:"result"`
}

// Minimal RPC config - one knob
type RPCConfig struct {
    URL            string `yaml:"url"`
    MaxParallelism int    `yaml:"max_parallelism"`
}
```

## Trace Fetching Strategy

1. **Try debug_traceBlockByNumber first** - fetches all traces for a block in one call
2. **Fallback to debug_traceTransaction** - per-transaction if block trace fails
3. **Handle precompile errors** - "incorrect number of top-level calls" returns nil trace (expected)
4. **Batch size for debug calls** - capped at 20 to avoid overwhelming debug endpoint

## Things That Were Planned But Not Implemented

1. Prometheus metrics - not added yet
2. Multiple RPCs per chain with load balancing - only uses first RPC
3. Full graceful shutdown - components have Stop() methods but main.go just blocks forever
4. StateDiffs fetching - field exists in NormalizedBlock but never populated

## Cloudflare R2 Support

Works with any S3-compatible storage. For R2:
- Set `s3_endpoint` to R2 URL
- Set `s3_region` to "auto"
- Provide R2 access key and secret
- Uses path-style addressing

## Client Library Usage

```go
// List chains
chains, _ := client.GetChains(ctx, "localhost:9090")

// Stream blocks with channel API
c := client.NewClient("localhost:9090", chainID)
blocks, errs := c.StreamBlocks(ctx, fromBlock)
for block := range blocks {
    // process block.Data (json.RawMessage)
}

// Stream blocks with handler API (auto-reconnects)
c := client.NewClient("localhost:9090", chainID, client.WithReconnect(true))
err := c.Stream(ctx, client.StreamConfig{FromBlock: 1}, func(chainID, blockNum uint64, data json.RawMessage) error {
    // process block
    return nil
})
```

## Performance Characteristics

- Ingestion: ~10-500 blocks/sec depending on RPC capacity and trace complexity
- Compaction: ~100 blocks every 10s once buffer fills
- Serving: Limited by network, not CPU (just streaming compressed JSON)
- Memory: Minimal - no large caches, streaming design
- S3 prefetching: 10 batches lookahead when serving historical data

## Dependencies

- github.com/cockroachdb/pebble/v2 - embedded KV store
- github.com/aws/aws-sdk-go-v2 - S3 client
- github.com/klauspost/compress/zstd - compression (both storage and TCP)
- github.com/gorilla/websocket - WebSocket for head tracking
- gopkg.in/yaml.v3 - config parsing
