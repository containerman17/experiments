# EVM Sink - Technical Notes for AI Context

## What This Is

A Go service that ingests EVM blockchain data (blocks, receipts, traces) from RPC nodes, stores in PebbleDB, 
compacts to S3, and serves to consumers via HTTP + WebSocket with zstd-compressed frames. Designed for multi-chain support.

## Architecture Decisions

### Storage Strategy: PebbleDB → S3

1. **PebbleDB (hot)**: All blocks land here first
   - Key format: `block:{chainID}:{blockNum:020d}` (20-digit padding for lexicographic ordering)
   - Value: JSON blob of NormalizedBlock (block + receipts + traces)
   - Keeps ~1000 blocks as buffer before compaction

2. **S3 (cold)**: Compacted historical data
   - Format: `.jsonl.zstd` (newline-delimited JSON, zstd compressed)
   - 100 blocks per file
   - Path: `{prefix}/{chainID}/{startBlock:020d}-{endBlock:020d}.jsonl.zstd`

3. **Why this split?**
   - PebbleDB for fast writes and recent block access
   - S3 for cheap, durable historical storage
   - Compaction happens in background, doesn't block ingestion

### Adaptive RPC Controller

Problem: RPC nodes have varying capacities, network latencies differ, load fluctuates.

Solution: One knob - `max_parallelism`. Everything else derived:
- `min_parallelism` = max(2, maxP/10)
- `target_latency` = 1200ms (allows 200ms ping + 1s work)
- `max_latency` = 2000ms
- `max_errors_per_min` = 10

Adjustment runs every 2s based on 60s sliding window:
- If errors > threshold: halve parallelism (aggressive backoff)
- If P95 > max_latency: reduce by 2
- If P95 < target: increase by 1

### WebSocket Head Tracker

The `HeadTracker` (rpc/heads.go) subscribes to `newHeads` via WebSocket for instant block notifications:
- On startup: fetches initial block via HTTP RPC
- Then subscribes to `eth_subscribe("newHeads")` via WebSocket
- Latest block is cached in atomic - `GetLatestBlock()` is instant, no RPC call
- Auto-reconnects on disconnect with 5s delay
- **Requires** `/rpc` in URL which gets converted to `/ws` for WebSocket

### Sliding Window Block Fetcher

The `Fetcher.StreamBlocks()` uses a sliding window approach:
- Keeps `windowSize` fetches in flight simultaneously
- Processes results in order as they complete
- Polls for new blocks every 100ms when at tip
- Window size configurable via `lookahead` config (default: 100)

### HTTP + WebSocket Streaming Protocol

**Endpoints:**
- `GET /chains` → HTTP JSON response (list available chains)
- `GET /ws?chain={id}&from={block}` → WebSocket upgrade, binary frames only

**WebSocket frames:** Binary only - `zstd(NormalizedBlock\n...)`, 1 to 100 blocks per frame

**Historical:** S3 blob sent as-is (already compressed JSONL with ~100 blocks)
**Live:** Each block compressed individually

Frame contents (after decompression, split on `\n`):
- Each line is a raw `NormalizedBlock` JSON
- Block number extracted from `block.number` field (hex encoded)
- No wrappers, no status messages - just blocks

### Streaming Logic (Server)

1. **Try PebbleDB first**: If block exists in hot storage, compress and send immediately
2. **Fallback to S3**: Send raw S3 blob (already compressed) with 10-batch lookahead prefetching
3. **At tip**: Poll PebbleDB every 50ms for new blocks
4. **Client handles first-frame alignment**: Filters blocks < fromBlock (S3 batches may start earlier)

### Ingestion Logic

1. Startup: Resume from PebbleDB latest → S3 latest → block 1
2. WebSocket subscription provides instant latest block (no polling needed)
3. Sliding window fetches blocks in parallel, processes in order
4. Save to PebbleDB, notify server of new blocks
5. Log progress every 5s with blocks/sec, ETA, parallelism, P95 latency
6. On any error: stop and restart from last saved block (no partial data)

### Compaction

- Runs every 3s per chain
- Only compacts if >1100 blocks in PebbleDB (keeps 1000 as buffer)
- Aligns to 100-block boundaries (1-100, 101-200, etc.)
- Uploads to S3, then deletes from PebbleDB

## File Structure

```
evm-sink/
├── cmd/
│   ├── sink/main.go          # Entry point, config, wiring, ingestion loop
│   └── example-client/main.go # Example client with reconnection and stats
├── consts/
│   └── consts.go             # All tunable constants (RPC*, Fetcher*, Storage*, Server*)
├── rpc/
│   ├── types.go              # EVM types (Block, Transaction, Receipt, etc.) + Config
│   ├── controller.go         # Adaptive parallelism controller with semaphore
│   ├── fetcher.go            # Sliding window block fetcher with batch RPC
│   └── heads.go              # WebSocket head tracker (newHeads subscription)
├── storage/
│   ├── pebble.go             # PebbleDB operations
│   ├── s3.go                 # S3 upload/download with zstd, batch helpers
│   └── compactor.go          # Background compaction
├── api/
│   └── server.go             # HTTP + WebSocket server with zstd frames
├── client/
│   └── client.go             # Go client library for consumers
├── config.yaml               # Local config (gitignored)
├── config.example.yaml       # Template
└── go.mod
```

## Config

```yaml
pebble_path: ./data/pebble
lookahead: 200  # Sliding window size for block fetching

s3_bucket: bucket-name
s3_region: auto
s3_endpoint: https://xxx.r2.cloudflarestorage.com  # R2/MinIO
s3_access_key: ...
s3_secret_key: ...
s3_prefix: v1

chains:
  # Avalanche C-Chain
  - chain_id: 43114
    name: C-Chain
    rpcs:
      - url: http://avalanche-node:9650/ext/bc/C/rpc
        max_parallelism: 200

  # Avalanche L1 subnets - URL format: /ext/bc/{blockchainID}/rpc
  - chain_id: 836
    name: BnryMainnet
    rpcs:
      - url: http://avalanche-node:9650/ext/bc/J3MYb3rDARLmB7FrRybinyjKqVTqmerbCr9bAXDatrSaHiLxQ/rpc
```

## Key Types

```go
// What we store per block
type NormalizedBlock struct {
    Block      Block                 `json:"block"`
    Traces     []TraceResultOptional `json:"traces"`
    StateDiffs []StateDiffResult     `json:"stateDiffs"`
    Receipts   []Receipt             `json:"receipts"`
}

// Trace result (nil Result for precompile calls)
type TraceResultOptional struct {
    TxHash string     `json:"txHash"`
    Result *CallTrace `json:"result"`
}

// Minimal RPC config - one knob
type RPCConfig struct {
    URL            string `yaml:"url"`
    MaxParallelism int    `yaml:"max_parallelism"`
}
```

## Trace & State Diff Fetching Strategy

**Call Traces:**
1. **Try debug_traceBlockByNumber first** - fetches all traces for a block in one call
2. **Fallback to debug_traceTransaction** - per-transaction if block trace fails
3. **Handle precompile errors** - "incorrect number of top-level calls" returns nil trace (expected)

**State Diffs:**
- Uses `debug_traceTransaction` with `prestateTracer` and `diffMode: true`
- Returns pre/post state for each address touched by the transaction
- Batched per-transaction (no block-level equivalent)

**Batch size for debug calls** - capped to avoid overwhelming debug endpoint (see `consts.FetcherDebugBatchSizeMax`)

## Things That Were Planned But Not Implemented

1. Prometheus metrics - not added yet
2. Multiple RPCs per chain with load balancing - only uses first RPC
3. Full graceful shutdown - components have Stop() methods but main.go just blocks forever

## Cloudflare R2 Support

Works with any S3-compatible storage. For R2:
- Set `s3_endpoint` to R2 URL
- Set `s3_region` to "auto"
- Provide R2 access key and secret
- Uses path-style addressing

## Client Library Usage

```go
// List chains via HTTP
chains, _ := client.GetChains(ctx, "localhost:9090")

// Stream blocks with channel API (WebSocket)
c := client.NewClient("localhost:9090", chainID)
blocks, errs := c.StreamBlocks(ctx, fromBlock)
for block := range blocks {
    // block.Number is the block number
    // block.Data is *rpc.NormalizedBlock (fully parsed)
}

// Stream blocks with handler API (auto-reconnects)
c := client.NewClient("localhost:9090", chainID, client.WithReconnect(true))
err := c.Stream(ctx, client.StreamConfig{FromBlock: 1}, func(blockNum uint64, block *rpc.NormalizedBlock) error {
    // block is fully parsed NormalizedBlock
    return nil
})
```

## Performance Characteristics

- Ingestion: ~10-500 blocks/sec depending on RPC capacity and trace complexity
- Compaction: ~100 blocks every 10s once buffer fills
- Serving: Limited by network, not CPU (just streaming compressed JSON)
- Memory: Minimal - no large caches, streaming design
- S3 prefetching: 10 batches lookahead when serving historical data
- Compression: ~10:1 for historical (S3 batches), ~3:1 for live blocks

## Dependencies

- github.com/cockroachdb/pebble/v2 - embedded KV store
- github.com/aws/aws-sdk-go-v2 - S3 client
- github.com/klauspost/compress/zstd - compression (storage and WebSocket frames)
- github.com/gorilla/websocket - WebSocket for head tracking and client serving
- gopkg.in/yaml.v3 - config parsing
